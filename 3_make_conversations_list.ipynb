{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. make conversation list\n",
    "Create conversation pickle file that contains all conversation according to definitation  \n",
    "**input**: database  \n",
    "**output**: conversation pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import time\n",
    "import pickle\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "airlines = {56377143: 'KLM', 106062176: 'Air France', 18332190: 'British Airways', 22536055: 'American Airlines', \n",
    "            124476322: 'Lufthansa', 26223583: 'Air Berlin', 2182373406: 'Air Berlin Assist', 38676903: 'easyJet', \n",
    "            1542862735: 'Ryanair', 253340062: 'Singapore Airlines', 218730857: 'Qantas',\n",
    "            45621423: 'Etihad Airways', 20626359: 'Virgin Atlantic'}\n",
    "\n",
    "# File you want to get the data from. Should be a database.\n",
    "file = 'core.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LoadDatabase(file, all_tweets):\n",
    "    '''in: database file (.db), all_tweets (boolean, True of False)\n",
    "      out: a dataframe with either all tweets, or tweets that are replies\n",
    "      \n",
    "    This function loads a database into a Pandas DataFrame.\n",
    "    With this function, you can choose whether you want a Pandas dataframe with all tweets from the database,\n",
    "    or only the tweets that are replies. It returns only the necessary variables from the tweets. \n",
    "    '''\n",
    "    stamp1 = time.time()\n",
    "    conn = sqlite3.connect(file)\n",
    "\n",
    "    query_all = '''\n",
    "    select id, created_at, text\n",
    "    from twitter\n",
    "    '''\n",
    "    \n",
    "    query_replies = '''\n",
    "    select id, user_id, in_reply_to_status_id, in_reply_to_user_id, created_at, text\n",
    "    from twitter\n",
    "    where in_reply_to_status_id IS NOT NULL AND in_reply_to_status_id != \"\"\n",
    "    '''\n",
    "    \n",
    "    if all_tweets:\n",
    "        dataframe = pd.read_sql(query_all, conn)\n",
    "    else:\n",
    "        dataframe = pd.read_sql(query_replies, conn)\n",
    "    \n",
    "    dataframe.set_index('id', inplace=True)\n",
    "    \n",
    "    stamp2 = time.time()\n",
    "    print('{:>7.3f} (read in database)'.format(stamp2-stamp1))\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BinarySearch(alist, item):\n",
    "    '''in: a list of numbers, the number to find (in our case, 'in_reply_to_status_id')\n",
    "      out: whether the number has been found (boolean). If yes: the index of that number\n",
    "      \n",
    "    This function performs binary search on an ordered list to find an element in O(log n) time.\n",
    "    Please note that the elements of the list are conversations, which exist of tweets. The functions\n",
    "    looks at the first element of every tweet.\n",
    "    '''\n",
    "    first = 0\n",
    "    last = len(alist)-1\n",
    "    found = False\n",
    "\n",
    "    while first<=last and not found:\n",
    "        midpoint = (first + last)//2\n",
    "        if alist[midpoint][0][0] == item:\n",
    "            found = True\n",
    "        else:\n",
    "            if item < alist[midpoint][0][0]:\n",
    "                last = midpoint-1\n",
    "            else:\n",
    "                first = midpoint+1\n",
    "\n",
    "    return found, midpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MakeConversations(tweets, *lowest_tweet_id):\n",
    "    '''in: a dataframe\n",
    "      out: list of basic 'conversations'\n",
    "      \n",
    "    The iterator goes in chronological order. When it finds a tweet that mentions another, it will add the following to \n",
    "    the conversations list: [(current_tweet_id, current_user_id, created_at, text), (mentioned_tweet_id, mentioned_used_id)]\n",
    "    or as an example: [(34839, 12, 23th May 12:34, 'your customer service is bad'), (32322, 14)]\n",
    "    \n",
    "    When the Binary Search function finds the mentioned tweet, it inserts the following information to a list described\n",
    "    above at the front of the conversation: (current_tweet_id, current_user_id, created_at, text). \n",
    "    For example, the earlier mentioned list now becomes:\n",
    "    [(35997, 124476322, 23th May 12:51, 'Thank you for your message, we are aware of that.'), \n",
    "     (34839, 12, 23th May 12:34, 'your customer service is bad'), (32322, 14)]\n",
    "    \n",
    "    This only holds if the id of the mentioned tweet is HIGHER than the LOWEST tweet id in a given set. Because why look for\n",
    "    tweet 5429 when the lowest in your set is 9000. Also note that the mentioned tweet lacks information. We will look\n",
    "    up this information in another function. In every conversation, there will be exactly one tweet lacking this information.\n",
    "    '''\n",
    "    stamp1 = time.time()\n",
    "    \n",
    "    if not lowest_tweet_id:\n",
    "        lowest_tweet_id = int(tweets.head(n=1).index.values[0])\n",
    "        \n",
    "    convos = []\n",
    "\n",
    "    for tweet_id, tweet in tweets.iterrows():\n",
    "        if tweet['in_reply_to_status_id'] >= lowest_tweet_id:\n",
    "            if len(convos) == 0:\n",
    "                convos.append([(tweet_id, tweet['user_id'], tweet['created_at'], tweet['text']), \n",
    "                               (tweet['in_reply_to_status_id'], tweet['in_reply_to_user_id'])])\n",
    "            else:\n",
    "                found, midpoint = BinarySearch(convos, tweet['in_reply_to_status_id'])\n",
    "\n",
    "                if found:\n",
    "                    convos[midpoint].insert(0, (tweet_id, tweet['user_id'], tweet['created_at'], tweet['text']))\n",
    "                    convos.append(convos[midpoint])\n",
    "                    del convos[midpoint]\n",
    "\n",
    "                else:\n",
    "                    convos.append([(tweet_id, tweet['user_id'], tweet['created_at'], tweet['text']),\n",
    "                                   (tweet['in_reply_to_status_id'], tweet['in_reply_to_user_id'])])\n",
    "    stamp2 = time.time()\n",
    "    print('{:>7.3f} (make conversations list)'.format(stamp2-stamp1))\n",
    "\n",
    "    return convos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FilterConversations(all_conversations):\n",
    "    '''in: a list of unprocessed 'conversations' (see MakeConversations() function)\n",
    "      out: a list of processed conversations\n",
    "      \n",
    "    This function processes all unprocessed conversations. With unprocessed, we mean all conversations of length bigger than \n",
    "    or equal to 2. It first checks whether the conversation has at least length 3, as we have defined them to be possible\n",
    "    conversations. Then, it checks the rule we have defined for a conversations to be an actual conversation.\n",
    "    \n",
    "    Rule: if there is an airline somewhere in the middle part, we consider the whole list as a conversation, e.g.;\n",
    "    [some other stuff]* ([some user] [airline] [some user])^1 [some other stuff]*\n",
    "    Note that only the middle part is absolutely necessary.\n",
    "    '''\n",
    "    stamp1 = time.time()\n",
    "    conversations_with_airlines = []\n",
    "\n",
    "    for conversation in all_conversations:\n",
    "        if len(conversation) > 2:\n",
    "            # It takes the middle part, so it trims the first and last tweet off\n",
    "            middle = conversation[1:-1]\n",
    "            # Check every tweet in \"middle\" part\n",
    "            for tweet_id in middle:\n",
    "                # Check if user is an airline\n",
    "                if int(tweet_id[1]) in airlines.keys():\n",
    "                    conversations_with_airlines.append(conversation)\n",
    "                    break\n",
    "\n",
    "    stamp2 = time.time()\n",
    "    print('{:>7.3f} (filter conversations list)'.format(stamp2-stamp1))\n",
    "    return conversations_with_airlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def CompleteTexts(conversations, all_tweets):\n",
    "    '''in: a list of filtered conversations, a dataframe of all the tweets from a database\n",
    "      out: a list of completed filtered conversations\n",
    "      \n",
    "    In the MakeConversations() functions, we talked about tweets that don't contain full information. Well, this function\n",
    "    retrieves that information and completes all the conversations.\n",
    "    '''\n",
    "    stamp1 = time.time()\n",
    "    to_delete = []\n",
    "\n",
    "    for con in range(len(conversations)):\n",
    "        for tweet in range(len(conversations[con])):\n",
    "            if len(conversations[con][tweet]) == 2:\n",
    "                try:\n",
    "                    tweet_info = all_tweets.loc[conversations[con][tweet][0]]\n",
    "                    text = tweet_info['text']\n",
    "                    created_at = tweet_info['created_at']\n",
    "                    conversations[con][tweet] = conversations[con][tweet] + (created_at, text)\n",
    "                # It deletes the conversation if the oldest tweet can't be found\n",
    "                except KeyError:\n",
    "                    to_delete.append(con)\n",
    "                    break\n",
    "                    \n",
    "    for index in sorted(to_delete, reverse=True):\n",
    "        del conversations[index]\n",
    "        \n",
    "    stamp2 = time.time()\n",
    "    print('{:>7.3f} (complete texts)'.format(stamp2-stamp1))\n",
    "    \n",
    "    return conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AddScores(conversations):\n",
    "    '''in: a list of completed filtered conversations\n",
    "      out: a list of completed filtered conversations with sentiment scores for all tweets\n",
    "      \n",
    "    This function uses the Vader package to add scores to all the tweets that are in conversations.\n",
    "    '''\n",
    "    stamp1 = time.time()\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    for conversation_index in range(len(conversations)):\n",
    "        for tweet_index in range(len(conversations[conversation_index])):\n",
    "            score = analyzer.polarity_scores(conversations[conversation_index][tweet_index][3])['compound']\n",
    "            conversations[conversation_index][tweet_index] = conversations[conversation_index][tweet_index] + (score,)\n",
    "\n",
    "    stamp2 = time.time()\n",
    "    print('{:>7.3f} (adding scores to {} conversations)'.format(stamp2 - stamp1, len(conversations)))\n",
    "    \n",
    "    return conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Combine(file):\n",
    "    '''in: a database file (.db)\n",
    "      out: a list of conversations with sentiment scores\n",
    "    \n",
    "    Bring everything together so you just need to input a database file.\n",
    "    \n",
    "    This whole thing takes approximately 10 minutes to run for the whole database, on a brick-like laptop that is arguably\n",
    "    fit to do computational stuff.\n",
    "    '''\n",
    "    stamp1 = time.time()\n",
    "    \n",
    "    all_tweets = LoadDatabase(file, True)\n",
    "    reply_tweets = LoadDatabase(file, False)\n",
    "    \n",
    "    all_conversations = MakeConversations(reply_tweets)\n",
    "    filtered_conversations = FilterConversations(all_conversations)\n",
    "    complete_conversations = CompleteTexts(filtered_conversations, all_tweets)\n",
    "    conversations_with_scores = AddScores(complete_conversations)\n",
    "    \n",
    "    stamp2 = time.time()\n",
    "    print('{:>7.3f} (total time)\\n'.format(stamp2 - stamp1))\n",
    "    \n",
    "    return conversations_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's hope it works\n",
    "conversations_with_scores = Combine(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store the conversations list in a pickle file in the obj folder\n",
    "pickle.dump(conversations_with_scores, open(\"obj/conversations_with_scores.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
